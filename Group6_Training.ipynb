{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "# --- Log everything to both console + file ---\n",
    "class Logger:\n",
    "    def __init__(self, filename):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(filename, \"w\", encoding=\"utf-8\")\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "    def flush(self):\n",
    "        self.terminal.flush()\n",
    "        self.log.flush()\n",
    "\n",
    "sys.stdout = sys.stderr = Logger(\"training_log.txt\")\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"10\"  # Adjust to half your cores\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"10\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"2\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d98d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- GPU Check ----\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"‚úÖ GPU detected: {gpus[0].name}\")\n",
    "        print(\"TensorFlow GPU memory growth enabled.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected ‚Äî running on CPU.\")\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "\n",
    "tf.config.threading.set_intra_op_parallelism_threads(6)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(2)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "print(\"OneDNN enabled:\", tf.config.optimizer.get_jit())\n",
    "\n",
    "# Deep Learning-based Classification of Benign and Malignant Melanoma using Ensemble Transfer Learning with Knowledge Distillation\n",
    "\n",
    "# ===============================\n",
    "# PARAMETERS\n",
    "# ===============================\n",
    "image_size = (224, 224)\n",
    "teacher_batch_size = 4\n",
    "student_batch_size = 4\n",
    "num_classes = 2\n",
    "\n",
    "# Change epochs here\n",
    "head_epochs = 50 # Train epoch: 15\n",
    "finetune_epochs = 100 # Train epoch = 25\n",
    "student_epochs = 250 # Train epoch = 50\n",
    "kf_splits = 5 # Kfold splits = 5\n",
    "\n",
    "\n",
    "save_dir = 'Training_Result_Test_2'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# CALLBACKS\n",
    "# ===============================\n",
    "teacher_callbacks = [\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "teacher_callbacks_head = [\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "teacher_callbacks_finetune = [\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "student_callbacks = [\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=17, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# TEACHER & STUDENT CONFIGURATION\n",
    "# ===============================\n",
    "teacher_models = {\n",
    "    # \"ResNet50\": tf.keras.applications.ResNet50,\n",
    "    # \"MobileNetV3Large\": tf.keras.applications.MobileNetV3Large,\n",
    "    # \"MobileNetV3Small\": tf.keras.applications.MobileNetV3Small,\n",
    "    # \"VGG16\": tf.keras.applications.VGG16\n",
    "    \"DenseNet121\": tf.keras.applications.DenseNet121,\n",
    "    #\"EfficientNetB0\": tf.keras.applications.EfficientNetB0,\n",
    "    # \"MobileNetV3\": tf.keras.applications.MobileNetV3Small\n",
    "    # \"Xception\": tf.keras.applications.Xception\n",
    "}\n",
    "\n",
    "def create_student():\n",
    "    base = tf.keras.applications.MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "    base.trainable = True\n",
    "    x = GlobalAveragePooling2D()(base.output)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    out = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=base.input, outputs=out, name=\"Student-MobileNetV2\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# üóÇÔ∏è Dataset Configuration\n",
    "# ===============================\n",
    "\n",
    "# Set the main dataset root folder here\n",
    "# dataset_root = \"test-dataset\"  \n",
    "dataset_root = \"test-dataset-train\" \n",
    "# Or: dataset_root = \"melanoma_cancer_dataset_Preprocessed\" # Original 10k data\n",
    "# Or: dataset_root = \"dataset\"\n",
    "\n",
    "# Automatically build all subpaths\n",
    "train_dir = os.path.join(dataset_root, \"data\", \"train\")\n",
    "test_dir = os.path.join(dataset_root, \"data\", \"test\")\n",
    "mask_dir = os.path.join(dataset_root, \"masks\")\n",
    "\n",
    "print(\"‚úÖ Using dataset root:\", dataset_root)\n",
    "print(\"   ‚î£ Train:\", train_dir)\n",
    "print(\"   ‚î£ Test :\", test_dir)\n",
    "print(\"   ‚îó Masks:\", mask_dir)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# ROBUST GRAD-CAM GENERATION + QUANTITATIVE EVALUATION\n",
    "# =====================================================\n",
    "def grad_cam(model, img_array, layer_name=None):\n",
    "\n",
    "    if len(img_array.shape) == 3:\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_tensor = tf.convert_to_tensor(img_array, dtype=tf.float32)\n",
    "\n",
    "    # Auto-detect last Conv2D layer\n",
    "    if layer_name is None:\n",
    "        for layer in reversed(model.layers):\n",
    "            if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "                layer_name = layer.name\n",
    "                break\n",
    "        if layer_name is None:\n",
    "            raise ValueError(\"No Conv2D layer found in model.\")\n",
    "\n",
    "    # Model outputs conv layer + predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        inputs=model.inputs,\n",
    "        outputs=[model.get_layer(layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(img_tensor)\n",
    "        conv_outputs, predictions = grad_model(img_tensor, training=False)\n",
    "        pred_index = tf.argmax(predictions[0])\n",
    "        class_channel = predictions[:, pred_index]\n",
    "\n",
    "    # Compute gradients\n",
    "    grads = tape.gradient(class_channel, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "\n",
    "    # Compute heatmap\n",
    "    heatmap = tf.reduce_mean(conv_outputs * pooled_grads, axis=-1)\n",
    "    heatmap = tf.maximum(heatmap, 0)\n",
    "    heatmap /= tf.reduce_max(heatmap) + 1e-8\n",
    "    heatmap = heatmap.numpy()\n",
    "\n",
    "    confidence = float(predictions[0][pred_index])\n",
    "    return heatmap, int(pred_index), confidence\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# IoU / Dice functions\n",
    "# -------------------------------\n",
    "def compute_iou(mask_gt, heatmap, threshold=0.5):\n",
    "    heatmap_bin = (heatmap >= threshold).astype(np.uint8)\n",
    "    intersection = np.logical_and(mask_gt, heatmap_bin).sum()\n",
    "    union = np.logical_or(mask_gt, heatmap_bin).sum()\n",
    "    if union == 0:\n",
    "        return np.nan\n",
    "    return intersection / union\n",
    "\n",
    "def compute_dice(mask_gt, heatmap, threshold=0.5):\n",
    "    heatmap_bin = (heatmap >= threshold).astype(np.uint8)\n",
    "    intersection = np.logical_and(mask_gt, heatmap_bin).sum()\n",
    "    total = mask_gt.sum() + heatmap_bin.sum()\n",
    "    if total == 0:\n",
    "        return np.nan\n",
    "    return 2 * intersection / total\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# ROBUST GRAD-CAM GENERATION + QUANTITATIVE EVALUATION\n",
    "# =====================================================\n",
    "def generate_and_eval_gradcam(model, test_gen, save_dir, all_teacher_models, classes=[\"benign\",\"malignant\"],\n",
    "                              image_size=(224,224), alpha=0.4, heatmap_threshold=0.4):\n",
    "    import matplotlib.pyplot as plt\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    ious, dices = [], []\n",
    "\n",
    "    for i, img_path in enumerate(tqdm(test_gen.filepaths, desc=\"Grad-CAM Eval\")):\n",
    "        # --- Load image\n",
    "        orig = cv2.imread(img_path)\n",
    "        if orig is None:\n",
    "            print(f\"‚ö†Ô∏è Could not read {img_path}, skipping\")\n",
    "            continue\n",
    "        orig_rgb = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # --- Preprocess image\n",
    "        img_resized = cv2.resize(orig_rgb, image_size) / 255.0\n",
    "        x_input = np.expand_dims(img_resized, axis=0)\n",
    "\n",
    "        # --- Grad-CAM\n",
    "        # heatmap, pred_idx, confidence = grad_cam(model, x_input)\n",
    "        # pred_label = classes[pred_idx]\n",
    "        # --- Grad-CAM on each teacher model\n",
    "        # heatmaps = []\n",
    "        # for idx, t_model in enumerate(all_teacher_models):\n",
    "        #     heatmap_t, pred_idx, confidence = grad_cam(t_model, x_input)\n",
    "        #     heatmaps.append(heatmap_t)\n",
    "\n",
    "        # # Average heatmaps for visualization if you want ensemble-like view\n",
    "        # heatmap = np.mean(np.stack(heatmaps, axis=0), axis=0)\n",
    "        # pred_label = classes[pred_idx]\n",
    "        # --- Grad-CAM on each teacher model\n",
    "        # heatmaps = []\n",
    "        # last_conf = 0\n",
    "        # last_idx = 0\n",
    "\n",
    "        # for idx, t_model in enumerate(all_teacher_models):\n",
    "        #     try:\n",
    "        #         heatmap_t, pred_idx, confidence = grad_cam(t_model, x_input)\n",
    "        #         heatmaps.append(heatmap_t)\n",
    "        #         last_conf = confidence\n",
    "        #         last_idx = pred_idx\n",
    "        #     except Exception as e:\n",
    "        #         print(f\"‚ö†Ô∏è GradCAM failed for model {idx}: {e}\")\n",
    "        \n",
    "        # # --- prevent crash if no model produced heatmap\n",
    "        # if len(heatmaps) == 0:\n",
    "        #     print(\"‚ùå No heatmaps generated for this image, skipping\")\n",
    "        #     continue\n",
    "        heatmaps = []\n",
    "        last_conf = 0\n",
    "        last_idx = 0\n",
    "\n",
    "        # Case 1: teacher models exist ‚Üí run GradCAM on teachers\n",
    "        if len(all_teacher_models) > 0:\n",
    "            for idx, t_model in enumerate(all_teacher_models):\n",
    "                try:\n",
    "                    heatmap_t, pred_idx, confidence = grad_cam(t_model, x_input)\n",
    "                    heatmaps.append(heatmap_t)\n",
    "                    last_conf = confidence\n",
    "                    last_idx = pred_idx\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è GradCAM failed for teacher model {idx}: {e}\")\n",
    "\n",
    "        # Case 2: no teachers ‚Üí run GradCAM on the provided model itself (student or ensemble)\n",
    "        else:\n",
    "            try:\n",
    "                heatmap_t, pred_idx, confidence = grad_cam(model, x_input)\n",
    "                heatmaps.append(heatmap_t)\n",
    "                last_conf = confidence\n",
    "                last_idx = pred_idx\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå GradCAM failed for model: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Prevent empty heatmap list\n",
    "        if len(heatmaps) == 0:\n",
    "            print(\"‚ùå No heatmaps generated for this image, skipping\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Average heatmaps to form ensemble heatmap\n",
    "        heatmap = np.mean(np.stack(heatmaps, axis=0), axis=0)\n",
    "        pred_label = classes[last_idx]\n",
    "        confidence = last_conf\n",
    "\n",
    "        # Save individual teacher GradCAM overlays\n",
    "        for idx, h in enumerate(heatmaps):\n",
    "            h_resized = cv2.resize(h, (orig.shape[1], orig.shape[0]))\n",
    "            h_uint8 = np.uint8(255 * h_resized)\n",
    "            h_color = cv2.applyColorMap(h_uint8, cv2.COLORMAP_JET)\n",
    "            overlay_t = cv2.addWeighted(orig_rgb, 1 - alpha, h_color, alpha, 0)\n",
    "\n",
    "            per_teacher_path = os.path.join(save_dir, f\"teacher_{idx}_gradcam_{i}.png\")\n",
    "            cv2.imwrite(per_teacher_path, cv2.cvtColor(overlay_t, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "\n",
    "        # Resize heatmap to original image size\n",
    "        heatmap_resized = cv2.resize(heatmap, (orig.shape[1], orig.shape[0]))\n",
    "\n",
    "        # --- Create Grad-CAM overlay\n",
    "        heatmap_uint8 = np.uint8(255 * heatmap_resized)\n",
    "        heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\n",
    "        gradcam_overlay = cv2.addWeighted(orig_rgb, 1 - alpha, heatmap_color, alpha, 0)\n",
    "\n",
    "        # --- Optional pseudo-mask for IoU/Dice\n",
    "        pseudo_mask = (heatmap_resized > heatmap_threshold).astype(np.uint8)\n",
    "        iou = compute_iou(pseudo_mask, pseudo_mask)  # trivial but placeholder\n",
    "        dice = compute_dice(pseudo_mask, pseudo_mask)\n",
    "        if not np.isnan(iou): ious.append(iou)\n",
    "        if not np.isnan(dice): dices.append(dice)\n",
    "\n",
    "        # --- Side-by-side plotting (Original vs Grad-CAM)\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12,6), facecolor=\"white\", constrained_layout=True)\n",
    "        fig.suptitle(f\"Prediction: {pred_label.capitalize()} ({confidence:.2f})\",\n",
    "                     fontsize=16, fontweight='bold', color='black', y=1.02)\n",
    "\n",
    "        axes[0].imshow(orig_rgb)\n",
    "        axes[0].set_title(\"Original\", fontsize=14, color='black', pad=10)\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        axes[1].imshow(gradcam_overlay)\n",
    "        axes[1].set_title(\"Grad-CAM\", fontsize=14, color='black', pad=10)\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        # --- Save visualization\n",
    "        out_file = os.path.join(save_dir, f\"gradcam_{i}.png\")\n",
    "        plt.savefig(out_file, bbox_inches=\"tight\", dpi=150, facecolor=\"white\")\n",
    "        plt.close(fig)\n",
    "        print(f\"‚úÖ Saved: {out_file}\")\n",
    "\n",
    "    # --- Average metrics\n",
    "    avg_iou = np.mean(ious) if len(ious) > 0 else np.nan\n",
    "    avg_dice = np.mean(dices) if len(dices) > 0 else np.nan\n",
    "    print(f\"‚úÖ Average Grad-CAM IoU: {avg_iou:.4f}\")\n",
    "    print(f\"‚úÖ Average Grad-CAM Dice: {avg_dice:.4f}\")\n",
    "\n",
    "    # --- Save metrics to txt\n",
    "    metrics_file = os.path.join(save_dir, \"gradcam_metrics.txt\")\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        f.write(f\"Average Grad-CAM IoU: {avg_iou:.4f}\\n\")\n",
    "        f.write(f\"Average Grad-CAM Dice: {avg_dice:.4f}\\n\")\n",
    "    print(f\"‚úÖ Metrics saved: {metrics_file}\")\n",
    " \n",
    "# ===============================\n",
    "# DATASET PREPARATION (for K-Fold)\n",
    "# ===============================\n",
    "# ---- Helper to check valid RGB image ----\n",
    "def is_rgb(path):\n",
    "    img = cv2.imread(path)\n",
    "    return img is not None and img.ndim == 3 and img.shape[2] == 3\n",
    "\n",
    "# ---- Collect all image paths ----\n",
    "all_images = glob(os.path.join(train_dir, '**', '*.*'), recursive=True)\n",
    "# Keep only common image file types (case-insensitive)\n",
    "all_images = [p for p in all_images if p.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "print(f\"üîç Found {len(all_images)} total image files before validation.\")\n",
    "\n",
    "# ---- Filter out non-RGB or corrupted images ----\n",
    "valid_images = [p for p in all_images if is_rgb(p)]\n",
    "invalid_count = len(all_images) - len(valid_images)\n",
    "\n",
    "if invalid_count > 0:\n",
    "    print(f\"‚ö†Ô∏è {invalid_count} improper or non-RGB images were removed.\")\n",
    "else:\n",
    "    print(\"‚úÖ All images are valid RGB.\")\n",
    "\n",
    "all_images = valid_images\n",
    "\n",
    "if len(all_images) == 0:\n",
    "    print(\"üö´ No valid images found! Please check your dataset structure (e.g., train/benign/, train/malignant/).\")\n",
    "\n",
    "# ---- Shuffle and create DataFrame ----\n",
    "all_images = shuffle(all_images, random_state=42)\n",
    "labels = [os.path.basename(os.path.dirname(p)) for p in all_images]\n",
    "df = pd.DataFrame({'filename': all_images, 'label': labels})\n",
    "\n",
    "# ---- Compute class weights ----\n",
    "classes = np.unique(df['label'])\n",
    "class_weights_values = compute_class_weight(class_weight='balanced', classes=classes, y=df['label'])\n",
    "class_weight_dict = {i: w for i, w in enumerate(class_weights_values)}\n",
    "print(\"‚öñÔ∏è Class weights:\", class_weight_dict)\n",
    "\n",
    "# ---- Prepare generators ----\n",
    "test_gen = ImageDataGenerator(rescale=1.0 / 255).flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=teacher_batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    color_mode='rgb'\n",
    ")\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1.0 / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd84634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# MODEL HELPERS\n",
    "# ===============================\n",
    "def create_base_model(base_model_func, name):\n",
    "    base = base_model_func(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "    base.trainable = False\n",
    "    x = GlobalAveragePooling2D()(base.output)\n",
    "    x = Dense(512, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    out = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=base.input, outputs=out, name=name)\n",
    "    model.compile(optimizer=Adam(1e-4), \n",
    "                  loss=\"categorical_crossentropy\", \n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# ===============================\n",
    "# KNOWLEDGE DISTILLATION CLASS\n",
    "# ===============================\n",
    "class Distiller(Model):\n",
    "    def __init__(self, student, teacher, **kwargs):\n",
    "        super().__init__(**kwargs)  \n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "\n",
    "    def compile(self, optimizer, metrics, student_loss_fn, distillation_loss_fn, alpha=0.1, temperature=5):\n",
    "        super().compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # --- Robust unpacking (handles 2 or 3 elements) ---\n",
    "        if isinstance(data, tuple):\n",
    "            if len(data) == 2:\n",
    "                x, y = data\n",
    "                sample_weight = None\n",
    "            elif len(data) == 3:\n",
    "                x, y, sample_weight = data\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected number of elements in data: {len(data)}\")\n",
    "        else:\n",
    "            # In case data is dict-like\n",
    "            x = data.get(\"x\") if isinstance(data, dict) and \"x\" in data else data[0]\n",
    "            y = data.get(\"y\") if isinstance(data, dict) and \"y\" in data else data[1]\n",
    "            sample_weight = None\n",
    "\n",
    "        # --- Teacher forward pass (frozen) ---\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # --- Student forward pass ---\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # --- Student loss (normal supervised) ---\n",
    "            if sample_weight is not None:\n",
    "                student_loss = self.student_loss_fn(y, student_predictions, sample_weight=sample_weight)\n",
    "            else:\n",
    "                student_loss = self.student_loss_fn(y, student_predictions)\n",
    "\n",
    "            # --- Distillation loss (soft targets) ---\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                tf.nn.sigmoid(teacher_predictions / self.temperature),\n",
    "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "            )\n",
    "\n",
    "            # --- Total loss = weighted sum ---\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "        # --- Backpropagation ---\n",
    "        grads = tape.gradient(loss, self.student.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.student.trainable_variables))\n",
    "\n",
    "        # --- Metrics update ---\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\n",
    "            \"student_loss\": student_loss,\n",
    "            \"distillation_loss\": distillation_loss\n",
    "        })\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # --- Robust unpacking for evaluation ---\n",
    "        if isinstance(data, tuple):\n",
    "            if len(data) == 2:\n",
    "                x, y = data\n",
    "                sample_weight = None\n",
    "            elif len(data) == 3:\n",
    "                x, y, sample_weight = data\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected number of elements in data: {len(data)}\")\n",
    "        else:\n",
    "            x = data.get(\"x\") if isinstance(data, dict) and \"x\" in data else data[0]\n",
    "            y = data.get(\"y\") if isinstance(data, dict) and \"y\" in data else data[1]\n",
    "            sample_weight = None\n",
    "\n",
    "        y_pred = self.student(x, training=False)\n",
    "        if sample_weight is not None:\n",
    "            student_loss = self.student_loss_fn(y, y_pred, sample_weight=sample_weight)\n",
    "        else:\n",
    "            student_loss = self.student_loss_fn(y, y_pred)\n",
    "\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# ENSEMBLE TEACHER (dynamic averaging)\n",
    "# ===============================\n",
    "class EnsembleTeacher(tf.keras.Model):\n",
    "    def __init__(self, models, **kwargs):\n",
    "        super().__init__(**kwargs)   # ‚úÖ don‚Äôt pass 'models' to super()\n",
    "        self.models = models\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = [model(inputs, training=training) for model in self.models]\n",
    "        # Average predictions\n",
    "        return tf.reduce_mean(tf.stack(outputs, axis=0), axis=0)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# K-FOLD TRAINING + EVALUATION\n",
    "# ===============================\n",
    "# X = features (filenames), y = labels\n",
    "X = df['filename']\n",
    "y = df['label']\n",
    "\n",
    "kf = StratifiedKFold(n_splits=kf_splits, shuffle=True, random_state=42)\n",
    "teacher_results = {}\n",
    "fold_results = []\n",
    "all_teacher_models = []\n",
    "all_y_true_train = []\n",
    "all_y_pred_train = []\n",
    "\n",
    "\n",
    "for teacher_name, teacher_fn in teacher_models.items():\n",
    "    print(f\"\\nüßë‚Äçüè´ Training Teacher: {teacher_name}\")\n",
    "    teacher_start = time.time()\n",
    "\n",
    "    teacher_dir = os.path.join(save_dir, f\"Teacher_{teacher_name}\")\n",
    "    os.makedirs(teacher_dir, exist_ok=True)\n",
    "    \n",
    "    all_fold_histories = []\n",
    "    trained_teachers = []\n",
    "    val_accs, val_losses = [], []\n",
    "\n",
    "    # Loop over folds\n",
    "    for fold_no, (train_idx, val_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\nüìö {teacher_name} ‚Äî Fold {fold_no}\")\n",
    "        fold_start = time.time()\n",
    "\n",
    "        train_df = df.iloc[train_idx]\n",
    "        val_df = df.iloc[val_idx]\n",
    "\n",
    "        train_gen = datagen.flow_from_dataframe(\n",
    "            train_df, x_col='filename', y_col='label',\n",
    "            target_size=image_size, class_mode='categorical',\n",
    "            batch_size=teacher_batch_size, shuffle=True, color_mode = 'rgb'\n",
    "        )\n",
    "        val_gen = datagen.flow_from_dataframe(\n",
    "            val_df, x_col='filename', y_col='label',\n",
    "            target_size=image_size, class_mode='categorical',\n",
    "            batch_size=teacher_batch_size, shuffle=False, color_mode = 'rgb'\n",
    "        )\n",
    "\n",
    "        # Create teacher for this fold\n",
    "        teacher = create_base_model(teacher_fn, teacher_name)\n",
    "\n",
    "        # --- Stage 1: Freeze backbone ---\n",
    "        for layer in teacher.layers:\n",
    "            if hasattr(layer, 'trainable'):\n",
    "                layer.trainable = False\n",
    "\n",
    "        teacher.compile(\n",
    "            optimizer=Adam(1e-3),\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "        history_head = teacher.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=head_epochs,\n",
    "            class_weight=class_weight_dict,\n",
    "            callbacks=teacher_callbacks_head,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # --- Stage 2: Unfreeze backbone ---\n",
    "        for layer in teacher.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        teacher.compile(\n",
    "            optimizer=Adam(1e-4),\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "        history_finetune = teacher.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=finetune_epochs,\n",
    "            class_weight=class_weight_dict,\n",
    "            callbacks=teacher_callbacks_finetune,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Append trained teacher for ensemble\n",
    "        trained_teachers.append(teacher)\n",
    "\n",
    "        # --- Merge full history ---\n",
    "        full_history = {}\n",
    "        for key in set(list(history_head.history.keys()) + list(history_finetune.history.keys())):\n",
    "            full_history[key] = history_head.history.get(key, []) + history_finetune.history.get(key, [])\n",
    "            \n",
    "        # --- Store full history per fold for averaging later ---\n",
    "        if 'all_fold_histories' not in locals():\n",
    "            all_fold_histories = []\n",
    "        all_fold_histories.append(full_history)\n",
    "\n",
    "        # --- Evaluate ---\n",
    "        val_loss, val_acc = teacher.evaluate(val_gen, verbose=1)\n",
    "        val_accs.append(val_acc)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        y_prob = teacher.predict(val_gen)\n",
    "        y_pred = np.argmax(y_prob, axis=1)\n",
    "        y_true = val_gen.classes\n",
    "\n",
    "        conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        report = classification_report(y_true, y_pred, target_names=val_gen.class_indices.keys())\n",
    "\n",
    "        # --- Save fold report ---\n",
    "        fold_report_path = os.path.join(teacher_dir, f\"fold_{fold_no}_report.txt\")\n",
    "        with open(fold_report_path, \"w\") as f:\n",
    "            f.write(f\"Model: {teacher_name}\\nFold: {fold_no}\\n\")\n",
    "            f.write(f\"Validation Loss: {val_loss:.4f}\\nValidation Accuracy: {val_acc:.4f}\\n\\n\")\n",
    "            f.write(\"Classification Report:\\n\" + report + \"\\n\")\n",
    "            f.write(\"Confusion Matrix:\\n\")\n",
    "            for row in conf_matrix:\n",
    "                f.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "        print(f\"‚úÖ Saved fold report: {fold_report_path}\")\n",
    "\n",
    "        # ===============================\n",
    "        # ===== Evaluate ON TRAIN SET ===\n",
    "        # ===============================\n",
    "        train_gen.reset()\n",
    "        y_prob_train = teacher.predict(train_gen, verbose=1)\n",
    "        y_pred_train = np.argmax(y_prob_train, axis=1)\n",
    "        y_true_train = train_gen.classes\n",
    "\n",
    "        # Append to overall lists\n",
    "        all_y_true_train.extend(y_true_train)\n",
    "        all_y_pred_train.extend(y_pred_train)\n",
    "\n",
    "        train_conf_matrix = confusion_matrix(y_true_train, y_pred_train)\n",
    "        train_report = classification_report(y_true_train, y_pred_train, target_names=train_gen.class_indices.keys())\n",
    "\n",
    "        train_report_path = os.path.join(teacher_dir, f\"fold_{fold_no}_TRAIN_report.txt\")\n",
    "        with open(train_report_path, \"w\") as f:\n",
    "            f.write(f\"TRAIN Classification Report for fold {fold_no}\\n\\n\")\n",
    "            f.write(train_report + \"\\n\")\n",
    "            f.write(\"Confusion Matrix:\\n\")\n",
    "            for row in train_conf_matrix:\n",
    "                f.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "        print(f\"üìÅ Saved TRAIN report: {train_report_path}\")\n",
    "\n",
    "        fold_end = time.time()\n",
    "        fold_duration = fold_end - fold_start\n",
    "        print(f\"‚è± Fold {fold_no} training time: {fold_duration/60:.2f} minutes\")\n",
    "\n",
    "\n",
    "        # --- Plot Accuracy & Loss ---\n",
    "        def get_key(hist, prefix=\"\", target=\"accuracy\"):\n",
    "            for k in hist:\n",
    "                if target in k and k.startswith(prefix):\n",
    "                    return k\n",
    "            return None\n",
    "\n",
    "        acc_key = get_key(full_history, target=\"accuracy\")\n",
    "        val_acc_key = get_key(full_history, prefix=\"val\", target=\"accuracy\")\n",
    "        loss_key = get_key(full_history, target=\"loss\")\n",
    "        val_loss_key = get_key(full_history, prefix=\"val\", target=\"loss\")\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        if acc_key:\n",
    "            plt.plot(full_history[acc_key], label=\"Train Accuracy\")\n",
    "        if val_acc_key:\n",
    "            plt.plot(full_history[val_acc_key], label=\"Val Accuracy\")\n",
    "        plt.title(f\"{teacher_name} - Fold {fold_no} Accuracy\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if loss_key:\n",
    "            plt.plot(full_history[loss_key], label=\"Train Loss\")\n",
    "        if val_loss_key:\n",
    "            plt.plot(full_history[val_loss_key], label=\"Val Loss\")\n",
    "        plt.title(f\"{teacher_name} - Fold {fold_no} Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(teacher_dir, f\"fold{fold_no}_training_curves.png\"))\n",
    "        plt.close()\n",
    "    \n",
    "    # --- Compute average learning curves across folds ---\n",
    "    max_epochs = max(len(h['accuracy']) for h in all_fold_histories)\n",
    "\n",
    "    avg_train_acc, avg_val_acc = [], []\n",
    "    avg_train_loss, avg_val_loss = [], []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        acc_vals, val_acc_vals, loss_vals, val_loss_vals = [], [], [], []\n",
    "        for h in all_fold_histories:\n",
    "            # handle folds with fewer epochs\n",
    "            acc_vals.append(h['accuracy'][epoch] if epoch < len(h['accuracy']) else np.nan)\n",
    "            val_acc_vals.append(h['val_accuracy'][epoch] if 'val_accuracy' in h and epoch < len(h['val_accuracy']) else np.nan)\n",
    "            loss_vals.append(h['loss'][epoch] if epoch < len(h['loss']) else np.nan)\n",
    "            val_loss_vals.append(h['val_loss'][epoch] if 'val_loss' in h and epoch < len(h['val_loss']) else np.nan)\n",
    "        avg_train_acc.append(np.nanmean(acc_vals))\n",
    "        avg_val_acc.append(np.nanmean(val_acc_vals))\n",
    "        avg_train_loss.append(np.nanmean(loss_vals))\n",
    "        avg_val_loss.append(np.nanmean(val_loss_vals))\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(avg_train_acc, label='Avg Train Acc', color='blue')\n",
    "    plt.plot(avg_val_acc, label='Avg Val Acc', color='orange')\n",
    "    plt.title(f\"{teacher_name} - Average Accuracy Across Folds\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(avg_train_loss, label='Avg Train Loss', color='blue')\n",
    "    plt.plot(avg_val_loss, label='Avg Val Loss', color='orange')\n",
    "    plt.title(f\"{teacher_name} - Average Loss Across Folds\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    avg_curve_path = os.path.join(teacher_dir, f\"{teacher_name}_avg_learning_curves.png\")\n",
    "    plt.savefig(avg_curve_path)\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Saved average learning curves across folds for {teacher_name} to: {avg_curve_path}\")\n",
    "\n",
    "    # ===============================\n",
    "    # FINAL SUMMARY (TEACHER REPORT)\n",
    "    # ===============================\n",
    "    print(f\"\\nüìä Generating final summary for {teacher_name}...\")\n",
    "    teacher_end = time.time()\n",
    "    teacher_time = teacher_end - teacher_start\n",
    "    print(f\"‚è± Total training time for {teacher_name}: {teacher_time/60:.2f} minutes ({teacher_time/3600:.2f} hours)\")\n",
    "\n",
    "    # Evaluate on test set for final report\n",
    "    test_loss, test_acc = teacher.evaluate(test_gen, verbose=1)\n",
    "\n",
    "    # ===============================\n",
    "    # TEST SET PREDICTION & CONF MATRIX\n",
    "    # ===============================\n",
    "    print(\"üîç Running predictions on TEST set...\")\n",
    "    # Ensure generator alignment and no shuffling\n",
    "    test_gen.reset()\n",
    "    test_gen.shuffle = False\n",
    "\n",
    "    y_prob_test = teacher.predict(test_gen, verbose=1)   # no steps argument\n",
    "    y_prob_test = y_prob_test[:test_gen.samples]\n",
    "# trim extra if any\n",
    "\n",
    "    y_pred_test = np.argmax(y_prob_test, axis=1)\n",
    "    y_true_test = test_gen.classes\n",
    "\n",
    "    print(\"y_true:\", len(y_true_test))\n",
    "    print(\"y_prob:\", y_prob_test.shape)\n",
    "\n",
    "    class_names = list(test_gen.class_indices.keys())\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_mat_test = confusion_matrix(y_true_test, y_pred_test)\n",
    "\n",
    "    # Save classification report\n",
    "    test_report = classification_report(y_true_test, y_pred_test, target_names=class_names, zero_division=0)\n",
    "    test_report_path = os.path.join(teacher_dir, f\"TEST_{teacher_name}_report.txt\")\n",
    "    with open(test_report_path, \"w\") as f:\n",
    "        f.write(f\"TEST Classification Report\\n\\n\")\n",
    "        f.write(test_report + \"\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        for row in conf_mat_test:\n",
    "            f.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "    print(f\"üìÅ Saved TEST report: {test_report_path}\")\n",
    "\n",
    "    # ---- Plot Confusion Matrix ---\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(conf_mat_test, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f\"{teacher_name} - Test Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(teacher_dir, f\"{teacher_name}_TEST_confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "    print(\"üìä Saved Test Confusion Matrix image\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Final Train Report for this teacher\n",
    "    # -----------------------------\n",
    "    final_train_report = classification_report(\n",
    "        all_y_true_train,\n",
    "        all_y_pred_train,\n",
    "        target_names=train_gen.class_indices.keys(),\n",
    "        zero_division=0\n",
    "    )\n",
    "    final_train_conf = confusion_matrix(all_y_true_train, all_y_pred_train)\n",
    "\n",
    "    final_train_report_path = os.path.join(teacher_dir, f\"FINAL_TRAIN_{teacher_name}_report.txt\")\n",
    "    with open(final_train_report_path, \"w\") as f:\n",
    "        f.write(f\"FINAL TRAIN Classification Report for {teacher_name} (All Folds)\\n\\n\")\n",
    "        f.write(final_train_report + \"\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        for row in final_train_conf:\n",
    "            f.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "    print(f\"üìÅ Saved FINAL TRAIN report for {teacher_name}: {final_train_report_path}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Final Train Confusion Matrix (heatmap)\n",
    "    # -----------------------------\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(final_train_conf, annot=True, fmt=\"d\",\n",
    "                xticklabels=list(train_gen.class_indices.keys()),\n",
    "                yticklabels=list(train_gen.class_indices.keys()))\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"FINAL TRAIN Confusion Matrix - {teacher_name}\")\n",
    "    plt.tight_layout()\n",
    "    conf_png_path = os.path.join(teacher_dir, f\"FINAL_TRAIN_{teacher_name}_confusion_matrix.png\")\n",
    "    plt.savefig(conf_png_path)\n",
    "    plt.close()\n",
    "    print(f\"üìÅ Saved FINAL TRAIN confusion matrix heatmap for {teacher_name}: {conf_png_path}\")\n",
    "\n",
    "\n",
    "    # Predict on validation generator\n",
    "    val_gen.reset()\n",
    "    y_true = val_gen.classes\n",
    "    y_prob = teacher.predict(val_gen, verbose=1)\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "    # Class names\n",
    "    target_names = list(val_gen.class_indices.keys())\n",
    "\n",
    "    # Compute classification metrics\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=target_names, zero_division=0)\n",
    "\n",
    "    # Average metrics across folds\n",
    "    avg_acc = np.mean(val_accs)\n",
    "    avg_loss = np.mean(val_losses)\n",
    "\n",
    "    # Save summary report\n",
    "    report_path = os.path.join(teacher_dir, f\"final_{teacher_name}_report.txt\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(f\"Average Validation Accuracy across folds: {avg_acc:.4f}\\n\")\n",
    "        f.write(f\"Average Validation Loss across folds: {avg_loss:.4f}\\n\")\n",
    "        f.write(f\"Test Accuracy: {test_acc:.4f}\\n\")\n",
    "        f.write(f\"Test Loss: {test_loss:.4f}\\n\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(report + \"\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        for row in conf_matrix:\n",
    "            f.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Final teacher report saved to: {report_path}\")\n",
    "\n",
    "    \n",
    "    # ===============================\n",
    "    # SAVE FINAL TRAINED TEACHER MODEL\n",
    "    # ===============================\n",
    "    final_teacher = teacher  # the last trained model after fine-tuning\n",
    "    final_teacher_path = os.path.join(teacher_dir, f\"final_{teacher_name}_model\")\n",
    "\n",
    "    try:\n",
    "        # Save in TensorFlow (Keras) format ‚Äî safer for modern TF versions\n",
    "        final_teacher.save(f\"{final_teacher_path}.keras\", include_optimizer=False)\n",
    "        print(f\"‚úÖ Saved final {teacher_name} model (Keras format) to: {final_teacher_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not save {teacher_name} using Keras save. Reason: {e}\")\n",
    "\n",
    "    # --- Explicit SavedModel export (replacement for .export()) ---\n",
    "    saved_teacher_dir = os.path.join(teacher_dir, f\"final_{teacher_name}_savedmodel\")\n",
    "    tf.saved_model.save(final_teacher, saved_teacher_dir)\n",
    "    print(f\"‚úÖ Exported TensorFlow SavedModel directory for {teacher_name} to: {saved_teacher_dir}\")\n",
    "\n",
    "    # --- Convert to TensorFlow Lite (for mobile / edge deployment) ---\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(saved_teacher_dir)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_teacher_model = converter.convert()\n",
    "\n",
    "    tflite_teacher_path = os.path.join(teacher_dir, f\"final_{teacher_name}_mobile.tflite\")\n",
    "    with open(tflite_teacher_path, \"wb\") as f:\n",
    "        f.write(tflite_teacher_model)\n",
    "\n",
    "    print(f\"‚úÖ Saved {teacher_name} TFLite model for mobile deployment to: {tflite_teacher_path}\")\n",
    "    \n",
    "    # --- Optional: free memory before next teacher model ---\n",
    "    tf.keras.backend.clear_session()\n",
    "    del teacher\n",
    "    gc.collect()\n",
    "\n",
    "    # Create full train generator (all training data)\n",
    "    full_train_gen = datagen.flow_from_dataframe(\n",
    "        dataframe=df,\n",
    "        x_col='filename',\n",
    "        y_col='label',\n",
    "        target_size=image_size,\n",
    "        class_mode='categorical',\n",
    "        batch_size=teacher_batch_size,\n",
    "        shuffle=True,\n",
    "        color_mode = 'rgb'\n",
    "    )\n",
    "\n",
    "    # After all folds for this teacher are trained\n",
    "    all_teacher_models.extend(trained_teachers)\n",
    "    # ===============================\n",
    "    # GRAD-CAM FOR THIS TEACHER MODEL\n",
    "    # ===============================\n",
    "    print(f\"üß† Generating Grad-CAM results for {teacher_name} ...\")\n",
    "\n",
    "    gradcam_output_dir = os.path.join(teacher_dir, f\"gradcam_{teacher_name}\")\n",
    "    os.makedirs(gradcam_output_dir, exist_ok=True)\n",
    "\n",
    "    # For GradCAM for EACH MODEL from each fold:\n",
    "    # for idx, t_model in enumerate(trained_teachers):\n",
    "    #     fold_gradcam_dir = os.path.join(gradcam_output_dir, f\"fold_{idx+1}\")\n",
    "    #     generate_and_eval_gradcam(\n",
    "    #         model=t_model,\n",
    "    #         test_gen=test_gen,\n",
    "    #         save_dir=fold_gradcam_dir,\n",
    "    #         all_teacher_models=all_teacher_models,\n",
    "    #         classes=[\"benign\", \"malignant\"],\n",
    "    #         image_size=image_size\n",
    "    #     )\n",
    "\n",
    "    # OR, if you want only final teacher:\n",
    "    generate_and_eval_gradcam(\n",
    "        model=final_teacher,\n",
    "        test_gen=test_gen,\n",
    "        save_dir=gradcam_output_dir,\n",
    "        all_teacher_models=all_teacher_models,\n",
    "        classes=[\"benign\", \"malignant\"],\n",
    "        image_size=image_size\n",
    "    )\n",
    "\n",
    "    print(f\"üî• Completed Grad-CAM for teacher: {teacher_name}\")\n",
    "\n",
    "    \n",
    "\n",
    "# ===============================\n",
    "# Create the Ensemble\n",
    "# ===============================\n",
    "ensemble_teacher = EnsembleTeacher(all_teacher_models)\n",
    "\n",
    "print(\"Teachers inside ensemble:\")\n",
    "for m in all_teacher_models:\n",
    "    print(m.name)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# DISTILLATION (Student)\n",
    "# ===============================\n",
    "print(\"‚úÖ Student Training Start\")\n",
    "student_start = time.time()\n",
    "\n",
    "\n",
    "student_dir = os.path.join(save_dir, \"Student_Ensemble\")\n",
    "os.makedirs(student_dir, exist_ok=True)\n",
    "\n",
    "student = create_student()\n",
    "distiller = Distiller(student=student, teacher=ensemble_teacher)\n",
    "distiller.compile(\n",
    "    optimizer=Adam(1e-4),\n",
    "    metrics=[\"accuracy\", Precision(name=\"precision\"), Recall(name=\"recall\"), AUC(name=\"auc\")],\n",
    "    student_loss_fn=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    distillation_loss_fn=tf.keras.losses.KLDivergence(),\n",
    "    alpha=0.7, # Change to 0.7 for future train\n",
    "    temperature=5 # Change to 5 for future train\n",
    ")\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, stratify=df['label'], random_state=42)\n",
    "\n",
    "train_gen_student = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=image_size,\n",
    "    class_mode='categorical',\n",
    "    batch_size=student_batch_size,\n",
    "    shuffle=True,\n",
    "    color_mode = 'rgb'\n",
    ")\n",
    "val_gen_student = datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=image_size,\n",
    "    class_mode='categorical',\n",
    "    batch_size=student_batch_size,\n",
    "    shuffle=False,\n",
    "    color_mode = 'rgb'\n",
    ")\n",
    "\n",
    "# Fit with explicit validation_data\n",
    "history_student = distiller.fit(\n",
    "    train_gen_student,\n",
    "    validation_data=val_gen_student,\n",
    "    epochs=student_epochs,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=student_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "results = distiller.evaluate(val_gen_student, return_dict=True)\n",
    "student_end = time.time()\n",
    "student_time = student_end - student_start\n",
    "print(f\"üéì Total Student training time: {student_time/60:.2f} minutes ({student_time/3600:.2f} hours)\")\n",
    "\n",
    "# -----------------------------\n",
    "# Assemble Keras Ensemble model (so it can be saved/loaded)\n",
    "# -----------------------------\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "# Make sure all_teacher_models is non-empty\n",
    "if len(all_teacher_models) == 0:\n",
    "    raise RuntimeError(\"No teacher models found in all_teacher_models to ensemble.\")\n",
    "\n",
    "# ================================================\n",
    "# FIX: Safely wrap each teacher with unique prefixes\n",
    "# ================================================\n",
    "renamed_teachers = []\n",
    "\n",
    "for i, teacher in enumerate(all_teacher_models):\n",
    "    x = tf.keras.Input(shape=(image_size[0], image_size[1], 3))\n",
    "    y = teacher(x, training=False)  # inference mode\n",
    "    model_wrapped = tf.keras.Model(x, y, name=f\"teacher_{i}\")\n",
    "    renamed_teachers.append(model_wrapped)\n",
    "\n",
    "# build ensemble model with safe names\n",
    "ensemble_input = tf.keras.Input(shape=(image_size[0], image_size[1], 3))\n",
    "teacher_outputs = [m(ensemble_input, training=False) for m in renamed_teachers]\n",
    "ensemble_avg = tf.keras.layers.Average(name=\"ensemble_average\")(teacher_outputs)\n",
    "\n",
    "ensemble_model = tf.keras.Model(ensemble_input, ensemble_avg, name=\"Ensembled_Teachers\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Built functional ensemble_model from trained teachers. Summary:\")\n",
    "ensemble_model.summary()\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate ensemble on test set and save metrics + visualizations\n",
    "# -----------------------------\n",
    "# Reset test generator\n",
    "test_gen.reset()\n",
    "y_true_test = test_gen.classes\n",
    "\n",
    "print(\"üîé Predicting with ensemble on test set...\")\n",
    "y_prob_test = ensemble_model.predict(test_gen, verbose=1)\n",
    "y_pred_test = np.argmax(y_prob_test, axis=1)\n",
    "\n",
    "# Classification report & confusion matrix\n",
    "ensemble_report = classification_report(y_true_test, y_pred_test, target_names=list(test_gen.class_indices.keys()), zero_division=0)\n",
    "ensemble_conf = confusion_matrix(y_true_test, y_pred_test)\n",
    "\n",
    "# Save textual report\n",
    "ensemble_report_path = os.path.join(save_dir, \"ensemble_test_report.txt\")\n",
    "with open(ensemble_report_path, \"w\") as f:\n",
    "    f.write(\"Ensemble Test Classification Report\\n\\n\")\n",
    "    f.write(ensemble_report + \"\\n\")\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    for row in ensemble_conf:\n",
    "        f.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "print(f\"‚úÖ Saved ensemble test report to: {ensemble_report_path}\")\n",
    "\n",
    "# Plot confusion matrix heatmap\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(ensemble_conf, annot=True, fmt=\"d\", xticklabels=list(test_gen.class_indices.keys()), yticklabels=list(test_gen.class_indices.keys()))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Ensemble - Confusion Matrix (Test)\")\n",
    "conf_png = os.path.join(save_dir, \"ensemble_confusion_matrix.png\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(conf_png)\n",
    "plt.close()\n",
    "print(f\"‚úÖ Saved ensemble confusion matrix heatmap to: {conf_png}\")\n",
    "\n",
    "# ROC curve & AUC (binary)\n",
    "try:\n",
    "    # use probability of positive class (class index 1)\n",
    "    if y_prob_test.shape[1] == 2:\n",
    "        pos_prob = y_prob_test[:, 1]\n",
    "        fpr, tpr, _ = skm.roc_curve(y_true_test, pos_prob)\n",
    "        roc_auc = skm.auc(fpr, tpr)\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\")\n",
    "        plt.plot([0,1],[0,1],\"--\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"Ensemble ROC Curve (Test)\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        roc_png = os.path.join(save_dir, \"ensemble_roc_curve.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(roc_png)\n",
    "        plt.close()\n",
    "        print(f\"‚úÖ Saved ensemble ROC curve to: {roc_png}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è ROC curve skipped - multi-class or unexpected output shape.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not compute ROC curve: {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Save ensemble model in multiple formats (SavedModel, .keras, TFLite)\n",
    "# -----------------------------\n",
    "ensemble_savedmodel_dir = os.path.join(save_dir, \"ensemble_savedmodel\")\n",
    "ensemble_keras_path = os.path.join(save_dir, \"ensemble_model.keras\")\n",
    "ensemble_tflite_path = os.path.join(save_dir, \"ensemble_model_mobile.tflite\")\n",
    "\n",
    "# SavedModel\n",
    "try:\n",
    "    tf.saved_model.save(ensemble_model, ensemble_savedmodel_dir)\n",
    "    print(f\"‚úÖ Exported ensemble SavedModel to: {ensemble_savedmodel_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save ensemble as SavedModel: {e}\")\n",
    "\n",
    "# .keras format\n",
    "try:\n",
    "    ensemble_model.save(ensemble_keras_path, include_optimizer=False)\n",
    "    print(f\"‚úÖ Saved ensemble (.keras) to: {ensemble_keras_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save ensemble .keras: {e}\")\n",
    "\n",
    "# TFLite conversion (from SavedModel)\n",
    "try:\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(ensemble_savedmodel_dir)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_ensemble = converter.convert()\n",
    "    with open(ensemble_tflite_path, \"wb\") as f:\n",
    "        f.write(tflite_ensemble)\n",
    "    print(f\"‚úÖ Saved ensemble TFLite model to: {ensemble_tflite_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not convert ensemble to TFLite: {e}\")\n",
    "\n",
    "print(\"üß† Running Grad-CAM for Ensemble Model ...\")\n",
    "\n",
    "ensemble_gradcam_dir = os.path.join(save_dir, \"gradcam_ensemble\")\n",
    "os.makedirs(ensemble_gradcam_dir, exist_ok=True)\n",
    "\n",
    "generate_and_eval_gradcam(\n",
    "    model=ensemble_model,\n",
    "    test_gen=test_gen,\n",
    "    save_dir=ensemble_gradcam_dir,\n",
    "    all_teacher_models=all_teacher_models,\n",
    "    classes=[\"benign\", \"malignant\"],\n",
    "    image_size=image_size\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Save the final STUDENT model (from distiller.student) similarly + visualizations\n",
    "# -----------------------------\n",
    "# After student is trained (i.e., after distiller.fit(...)), create final_student ref:\n",
    "final_student = distiller.student if hasattr(distiller, \"student\") else student\n",
    "\n",
    "# Evaluate student on test set\n",
    "test_gen.reset()\n",
    "y_prob_student = final_student.predict(test_gen, verbose=1)\n",
    "y_pred_student = np.argmax(y_prob_student, axis=1)\n",
    "student_conf = confusion_matrix(y_true_test, y_pred_student)\n",
    "student_report = classification_report(y_true_test, y_pred_student, target_names=list(test_gen.class_indices.keys()), zero_division=0)\n",
    "\n",
    "# Save student report\n",
    "student_report_path = os.path.join(save_dir, \"student_test_report.txt\")\n",
    "with open(student_report_path, \"w\") as f:\n",
    "    f.write(\"Student Test Classification Report\\n\\n\")\n",
    "    f.write(student_report + \"\\n\")\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    for row in student_conf:\n",
    "        f.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "print(f\"‚úÖ Saved student test report to: {student_report_path}\")\n",
    "\n",
    "# Plot student confusion matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(student_conf, annot=True, fmt=\"d\", xticklabels=list(test_gen.class_indices.keys()), yticklabels=list(test_gen.class_indices.keys()))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Student - Confusion Matrix (Test)\")\n",
    "student_conf_png = os.path.join(save_dir, \"student_confusion_matrix.png\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(student_conf_png)\n",
    "plt.close()\n",
    "print(f\"‚úÖ Saved student confusion matrix heatmap to: {student_conf_png}\")\n",
    "\n",
    "# Save student savedmodel / .keras / tflite\n",
    "final_student_savedmodel_dir = os.path.join(save_dir, \"final_student_savedmodel\")\n",
    "final_student_keras = os.path.join(save_dir, \"final_student_model.keras\")\n",
    "final_student_tflite = os.path.join(save_dir, \"final_student_mobile.tflite\")\n",
    "\n",
    "try:\n",
    "    tf.saved_model.save(final_student, final_student_savedmodel_dir)\n",
    "    print(f\"‚úÖ Exported final student SavedModel to: {final_student_savedmodel_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save student as SavedModel: {e}\")\n",
    "    # fallback to weights\n",
    "    try:\n",
    "        weights_path = os.path.join(save_dir, \"final_student_weights.h5\")\n",
    "        final_student.save_weights(weights_path)\n",
    "        print(f\"‚úÖ Saved student weights to: {weights_path}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ö†Ô∏è Also failed to save student weights: {e2}\")\n",
    "\n",
    "try:\n",
    "    final_student.save(final_student_keras, include_optimizer=False)\n",
    "    print(f\"‚úÖ Saved student (.keras) to: {final_student_keras}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save student .keras: {e}\")\n",
    "\n",
    "# Convert to TFLite\n",
    "try:\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(final_student_savedmodel_dir)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_student = converter.convert()\n",
    "    with open(final_student_tflite, \"wb\") as f:\n",
    "        f.write(tflite_student)\n",
    "    print(f\"‚úÖ Saved final student TFLite model to: {final_student_tflite}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not convert student to TFLite: {e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Run Grad-CAM pipeline\n",
    "# -------------------------------\n",
    "# for l in final_student.layers:\n",
    "#     print(l.name, l.output_shape, type(l))\n",
    "\n",
    "\n",
    "generate_and_eval_gradcam(\n",
    "    model=final_student,\n",
    "    test_gen=test_gen,\n",
    "    save_dir=os.path.join(save_dir, \"gradcam_results\"),\n",
    "    all_teacher_models=[],   # <-- add this\n",
    "    classes=[\"benign\", \"malignant\"],\n",
    "    image_size=image_size\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Minimal ensemble vs student summary file\n",
    "# -----------------------------\n",
    "summary_path = os.path.join(save_dir, \"ensemble_student_summary.txt\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    f.write(\"Ensemble vs Student - Test Summary\\n\\n\")\n",
    "    f.write(\"=== Ensemble Report ===\\n\")\n",
    "    f.write(ensemble_report + \"\\n\\n\")\n",
    "    f.write(\"=== Student Report ===\\n\")\n",
    "    f.write(student_report + \"\\n\\n\")\n",
    "    if 'roc_auc' in locals():\n",
    "        f.write(f\"Ensemble AUC: {roc_auc:.4f}\\n\")\n",
    "print(f\"‚úÖ Saved combined summary to: {summary_path}\")\n",
    "\n",
    "\n",
    "metrics = results.get(\"compile_metrics\", {})\n",
    "\n",
    "# Try to find total loss if available, otherwise fallback to student_loss\n",
    "val_loss = results.get(\"loss\", results.get(\"student_loss\", None))\n",
    "val_acc = results.get(\"accuracy\", results.get(\"student_accuracy\", 0))\n",
    "precision = results.get(\"precision\", results.get(\"student_precision\", 0))\n",
    "recall = results.get(\"recall\", results.get(\"student_recall\", 0))\n",
    "auc_value = results.get(\"auc\", results.get(\"student_auc\", 0))  # ‚úÖ rename here\n",
    "\n",
    "fold_results.append({\n",
    "    \"fold\": fold_no,\n",
    "    \"val_loss\": float(val_loss) if val_loss is not None else 0.0,\n",
    "    \"val_acc\": float(val_acc),\n",
    "    \"precision\": float(precision),\n",
    "    \"recall\": float(recall),\n",
    "    \"auc\": float(auc_value)  # ‚úÖ use the new name\n",
    "})\n",
    "print(f\"‚úÖ Fold {fold_no} results: {results}\")\n",
    "\n",
    "# --- Plot training curves per fold ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy subplot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_head.history[\"accuracy\"], label=\"Head Train Acc\", linestyle='--')\n",
    "plt.plot(history_head.history[\"val_accuracy\"], label=\"Head Val Acc\", linestyle='--')\n",
    "plt.plot(history_finetune.history[\"accuracy\"], label=\"Fine-tune Train Acc\", linestyle='-.')\n",
    "plt.plot(history_finetune.history[\"val_accuracy\"], label=\"Fine-tune Val Acc\", linestyle='-.')\n",
    "plt.title(f\"{teacher_name} - Fold {fold_no} Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# Loss subplot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_head.history[\"loss\"], label=\"Head Train Loss\", linestyle='--')\n",
    "plt.plot(history_head.history[\"val_loss\"], label=\"Head Val Loss\", linestyle='--')\n",
    "plt.plot(history_finetune.history[\"loss\"], label=\"Fine-tune Train Loss\", linestyle='-.')\n",
    "plt.plot(history_finetune.history[\"val_loss\"], label=\"Fine-tune Val Loss\", linestyle='-.')\n",
    "plt.title(f\"{teacher_name} - Fold {fold_no} Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, f\"fold{fold_no}_training_curves.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# POST-EVALUATION (AVERAGE METRICS)\n",
    "# ===============================\n",
    "mean_acc = np.mean([r[\"val_acc\"] for r in fold_results if r.get(\"val_acc\") is not None])\n",
    "print(f\"\\n‚úÖ Average Validation Accuracy across folds: {mean_acc:.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# FINAL EVALUATION ON TEST SET\n",
    "# ===============================\n",
    "print(\"\\n=== Final Evaluation on Test Set ===\")\n",
    "\n",
    "# Use the last trained student model\n",
    "final_student = student\n",
    "\n",
    "# üîß Compile student before evaluation\n",
    "final_student.compile(\n",
    "    optimizer=Adam(1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# EVALUATE & GENERATE PREDICTIONS\n",
    "# ===============================\n",
    "test_loss, test_acc = final_student.evaluate(test_gen, verbose=1)\n",
    "print(f\"‚úÖ Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"‚úÖ Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Reset generator and predict\n",
    "test_gen.reset()\n",
    "y_prob = final_student.predict(test_gen)\n",
    "\n",
    "# Handle both binary (sigmoid) and multi-class (softmax)\n",
    "if y_prob.shape[1] == 1:\n",
    "    y_score = y_prob.ravel()  # Binary sigmoid\n",
    "else:\n",
    "    y_score = y_prob  # Multi-class softmax\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "# Align lengths\n",
    "y_true = test_gen.classes[:len(y_pred)]\n",
    "y_pred = y_pred[:len(y_true)]\n",
    "if y_score.ndim > 1:\n",
    "    y_score = y_score[:len(y_true), :]\n",
    "\n",
    "# Target names\n",
    "target_names = list(test_gen.class_indices.keys())\n",
    "\n",
    "# Classification report & confusion matrix\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=target_names, zero_division=0))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# ===============================\n",
    "# SAVE FINAL STUDENT MODEL (FOR TESTING & DEPLOYMENT)\n",
    "# ===============================\n",
    "final_model_path = os.path.join(save_dir, \"final_student_model\")\n",
    "final_savedmodel_dir = os.path.join(save_dir, \"final_student_savedmodel\")\n",
    "final_tflite_path = os.path.join(save_dir, \"final_student_mobile.tflite\")\n",
    "\n",
    "try:\n",
    "    # ‚úÖ Save using TensorFlow SavedModel (recommended)\n",
    "    tf.saved_model.save(final_student, final_savedmodel_dir)\n",
    "    print(f\"‚úÖ Exported TensorFlow SavedModel directory to: {final_savedmodel_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save student as SavedModel: {e}\")\n",
    "    # Fallback: save weights only\n",
    "    weights_path = os.path.join(save_dir, \"final_student_weights.h5\")\n",
    "    final_student.save_weights(weights_path)\n",
    "    print(f\"‚úÖ Saved student weights only to: {weights_path}\")\n",
    "\n",
    "# ‚úÖ Optional ‚Äî also save as standard .keras model (Functional API compatible)\n",
    "try:\n",
    "    final_student.save(f\"{final_model_path}.keras\", include_optimizer=False)\n",
    "    print(f\"‚úÖ Saved student model (.keras) to: {final_model_path}.keras\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save .keras model: {e}\")\n",
    "\n",
    "# ===============================\n",
    "# CONVERT TO TENSORFLOW LITE\n",
    "# ===============================\n",
    "try:\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(final_savedmodel_dir)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    with open(final_tflite_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(f\"‚úÖ Saved TFLite model for mobile deployment to: {final_tflite_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not convert to TFLite: {e}\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# ROC-AUC (binary only)\n",
    "# ===============================\n",
    "if len(target_names) == 2:\n",
    "    # Use positive class probabilities for ROC\n",
    "    if y_score.shape[1] == 2:\n",
    "        y_score_pos = y_score[:, 1]\n",
    "    else:\n",
    "        y_score_pos = y_score  # Already shape (num_samples,)\n",
    "    \n",
    "    # Ensure no NaN\n",
    "    y_score_pos = np.nan_to_num(y_score_pos)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score_pos)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\")\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve - Final Student Model\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(save_dir, \"roc_curve.png\"))\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ ROC curve saved to: {os.path.join(save_dir, 'roc_curve.png')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è ROC-AUC is only computed for binary classification.\")\n",
    "\n",
    "# ===============================\n",
    "# CONFUSION MATRIX & CLASSIFICATION REPORT (SAVE)\n",
    "# ===============================\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred, target_names=target_names, zero_division=0)\n",
    "\n",
    "report_path = os.path.join(save_dir, \"final_student_report.txt\")\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(f\"Average Validation Accuracy across folds: {mean_acc:.4f}\\n\")\n",
    "    f.write(f\"Test Accuracy: {test_acc:.4f}\\n\")\n",
    "    f.write(f\"Test Loss: {test_loss:.4f}\\n\\n\")\n",
    "    f.write(\"Classification Report:\\n\")\n",
    "    f.write(report + \"\\n\")\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    for row in conf_matrix:\n",
    "        f.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Report saved to: {report_path}\")\n",
    "\n",
    "# ===============================\n",
    "# CONFUSION MATRIX (RAW)\n",
    "# ===============================\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    conf_matrix,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=test_gen.class_indices.keys(),\n",
    "    yticklabels=test_gen.class_indices.keys()\n",
    ")\n",
    "plt.title(\"Confusion Matrix - Final Student Model\")\n",
    "plt.savefig(os.path.join(save_dir, \"confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ===============================\n",
    "# CONFUSION MATRIX (NORMALIZED)\n",
    "# ===============================\n",
    "conf_matrix_norm = conf_matrix.astype(\"float\") / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    conf_matrix_norm,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"Greens\",\n",
    "    xticklabels=test_gen.class_indices.keys(),\n",
    "    yticklabels=test_gen.class_indices.keys()\n",
    ")\n",
    "plt.title(\"Normalized Confusion Matrix - Final Student Model\")\n",
    "plt.savefig(os.path.join(save_dir, \"confusion_matrix_normalized.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ===============================\n",
    "# PRECISION‚ÄìRECALL CURVE\n",
    "# ===============================\n",
    "if len(target_names) == 2:\n",
    "    # Get positive class probability\n",
    "    if y_prob.shape[1] == 2:\n",
    "        y_score_pos = y_prob[:, 1]\n",
    "    else:\n",
    "        y_score_pos = y_prob.ravel()  # Binary sigmoid case\n",
    "\n",
    "    # Remove NaNs\n",
    "    y_score_pos = np.nan_to_num(y_score_pos)\n",
    "\n",
    "    # Ensure y_true and y_score_pos have same length\n",
    "    min_len = min(len(y_true), len(y_score_pos))\n",
    "    y_true_trim = y_true[:min_len]\n",
    "    y_score_trim = y_score_pos[:min_len]\n",
    "\n",
    "    # Compute Precision‚ÄìRecall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true_trim, y_score_trim)\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(recall, precision, label=\"PR Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision‚ÄìRecall Curve - Student Model\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(save_dir, \"precision_recall_curve.png\"))\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Precision‚ÄìRecall curve saved to: {os.path.join(save_dir, 'precision_recall_curve.png')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Precision‚ÄìRecall curve is only computed for binary classification.\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# PER-CLASS PRECISION, RECALL, F1 BAR CHART\n",
    "# ===============================\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, zero_division=0)\n",
    "classes = list(test_gen.class_indices.keys())\n",
    "\n",
    "bar_width = 0.25\n",
    "r1 = np.arange(len(classes))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(r1, prec, width=bar_width, label=\"Precision\")\n",
    "plt.bar(r1 + bar_width, rec, width=bar_width, label=\"Recall\")\n",
    "plt.bar(r1 + 2 * bar_width, f1, width=bar_width, label=\"F1-score\")\n",
    "plt.xticks(r1 + bar_width, classes)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Per-Class Metrics\")\n",
    "plt.legend()\n",
    "plt.grid(axis=\"y\")\n",
    "plt.savefig(os.path.join(save_dir, \"class_metrics_bar.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ===============================\n",
    "# PREDICTION CONFIDENCE HISTOGRAM\n",
    "# ===============================\n",
    "confidences = np.nan_to_num(np.max(y_prob, axis=1))\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.hist(confidences, bins=20, color=\"purple\", alpha=0.7)\n",
    "plt.title(\"Prediction Confidence Distribution\")\n",
    "plt.xlabel(\"Confidence\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(save_dir, \"confidence_histogram.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ===============================\n",
    "# MISCLASSIFIED SAMPLES (OPTIONAL)\n",
    "# ===============================\n",
    "y_true_trim, y_pred_trim, y_prob_trim = y_true[:min_len], y_pred[:min_len], y_prob[:min_len]\n",
    "wrong_idx = np.where(y_pred_trim != y_true_trim)[0]\n",
    "\n",
    "if len(wrong_idx) > 0:\n",
    "    sample_wrong = np.random.choice(wrong_idx, min(9, len(wrong_idx)), replace=False)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    for i, idx in enumerate(sample_wrong):\n",
    "        img_path = test_gen.filepaths[idx]\n",
    "        img = plt.imread(img_path)\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(\n",
    "            f\"True: {list(test_gen.class_indices.keys())[y_true[idx]]}\\n\"\n",
    "            f\"Pred: {list(test_gen.class_indices.keys())[y_pred[idx]]}\"\n",
    "        )\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"misclassified_samples.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Calibration Curve    \n",
    "if len(target_names) == 2:\n",
    "    y_score_pos = np.nan_to_num(y_prob[:, 1])\n",
    "    prob_true, prob_pred = calibration_curve(y_true_trim, y_score_pos[:min_len], n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\", label=\"Calibration\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly Calibrated\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"True Probability\")\n",
    "plt.title(\"Reliability Curve (Calibration Plot)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(save_dir, \"calibration_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Cumulative Gain Curve\n",
    "if len(target_names) == 2:\n",
    "    y_score_pos = np.nan_to_num(y_prob[:, 1])\n",
    "    sorted_idx = np.argsort(y_score_pos[:min_len])[::-1]\n",
    "    y_true_sorted = np.array(y_true_trim)[sorted_idx]\n",
    "\n",
    "cum_positive_rate = np.cumsum(y_true_sorted) / np.sum(y_true_sorted)\n",
    "perc_samples = np.arange(1, len(y_true_sorted)+1) / len(y_true_sorted)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(perc_samples, cum_positive_rate, label=\"Model\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "plt.xlabel(\"Proportion of Samples\")\n",
    "plt.ylabel(\"Cumulative Proportion of Positives\")\n",
    "plt.title(\"Cumulative Gain Chart\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(save_dir, \"cumulative_gain_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ===============================\n",
    "# F1-SCORE vs DECISION THRESHOLD\n",
    "# ===============================\n",
    "# Ensure binary case\n",
    "if y_prob.shape[1] == 2:\n",
    "    # Binary classification\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    f1s = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_pred_thresh = (y_prob[:, 1] >= t).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred_thresh, zero_division=0)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(thresholds, f1s, color=\"teal\")\n",
    "    plt.xlabel(\"Decision Threshold\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(\"F1 Score vs Decision Threshold\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Mark optimal threshold\n",
    "    best_idx = np.argmax(f1s)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_f1 = f1s[best_idx]\n",
    "    plt.axvline(best_threshold, color=\"red\", linestyle=\"--\", label=f\"Best t = {best_threshold:.2f}\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(os.path.join(save_dir, \"f1_vs_threshold.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"‚úÖ F1 vs Threshold plot saved. Optimal threshold ‚âà {best_threshold:.2f}, F1 = {best_f1:.4f}\")\n",
    "\n",
    "else:\n",
    "    # Multi-class classification ‚Äî skip or use one-vs-rest\n",
    "    print(\"‚ö†Ô∏è F1 vs Threshold skipped (only valid for binary classification).\")\n",
    "    \n",
    "\n",
    "# ===============================\n",
    "# CLASS DISTRIBUTION vs CONFIDENCE\n",
    "# ===============================\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "# Map: class_index -> class_name\n",
    "idx_to_class = {v: k for k, v in test_gen.class_indices.items()}\n",
    "\n",
    "# Ensure y_prob and y_true exist\n",
    "if 'y_prob' not in locals():\n",
    "    y_prob = final_student.predict(test_gen)\n",
    "if 'y_true' not in locals():\n",
    "    y_true = np.array(test_gen.classes)\n",
    "\n",
    "# Iterate per class and plot histogram of predicted confidence\n",
    "for i, cls in idx_to_class.items():\n",
    "    # Predicted confidence for true samples of this class\n",
    "    cls_confidences = y_prob[y_true == i, i] if y_prob.shape[1] > i else np.array([])\n",
    "\n",
    "    # Remove NaNs\n",
    "    cls_confidences = cls_confidences[~np.isnan(cls_confidences)]\n",
    "\n",
    "    if len(cls_confidences) == 0:\n",
    "        print(f\"‚ö†Ô∏è No valid predictions for class '{cls}', skipping.\")\n",
    "        continue  # skip empty class\n",
    "\n",
    "    plt.hist(\n",
    "        cls_confidences,\n",
    "        bins=20,\n",
    "        alpha=0.6,\n",
    "        label=f\"{cls} (n={len(cls_confidences)})\",\n",
    "        density=True,  # normalize to make shape comparable\n",
    "        edgecolor=\"black\"\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Predicted Probability for True Class\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Confidence Distribution per Class\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.4)\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = os.path.join(save_dir, \"class_confidence_distribution.png\")\n",
    "plt.savefig(save_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"‚úÖ Class confidence distribution plot saved to: {save_path}\")\n",
    "\n",
    "# ===============================\n",
    "# K-FOLD METRICS BOXPLOT\n",
    "# ===============================\n",
    "# Convert results list of dicts into DataFrame\n",
    "metrics_df = pd.DataFrame(fold_results)\n",
    "\n",
    "if metrics_df.empty:\n",
    "    print(\"‚ö†Ô∏è fold_results is empty, skipping boxplot.\")\n",
    "else:\n",
    "    # --- Sanity check for available columns ---\n",
    "    available_cols = [c for c in [\"val_acc\", \"val_f1\"] if c in metrics_df.columns]\n",
    "    if len(available_cols) == 0:\n",
    "        raise ValueError(\"‚ö†Ô∏è No valid metric columns found in fold_results. Expected 'val_acc' or 'val_f1'.\")\n",
    "\n",
    "    # --- Boxplot ---\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    metrics_df.boxplot(column=available_cols)\n",
    "    plt.title(\"K-Fold Validation Metric Distribution\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.grid(axis=\"y\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # --- Save plot ---\n",
    "    save_path = os.path.join(save_dir, \"kfold_metrics_boxplot.png\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"‚úÖ K-Fold metrics boxplot saved to: {save_path}\")\n",
    "    print(f\"üìä Columns plotted: {available_cols}\")\n",
    "\n",
    "# ===============================\n",
    "# LEARNING CURVES (ACCURACY / LOSS)\n",
    "# ===============================\n",
    "# --- Accuracy Plots ---\n",
    "# Head accuracy\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(history_head.history[\"accuracy\"], label=\"Train Acc\", linestyle='--')\n",
    "plt.plot(history_head.history[\"val_accuracy\"], label=\"Val Acc\", linestyle='--')\n",
    "plt.title(\"Head Training Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, \"head_accuracy_curve.png\"))\n",
    "plt.close()\n",
    "print(\"‚úÖ Saved: head_accuracy_curve.png\")\n",
    "\n",
    "# Fine-tune accuracy\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(history_finetune.history[\"accuracy\"], label=\"Train Acc\", linestyle='-.')\n",
    "plt.plot(history_finetune.history[\"val_accuracy\"], label=\"Val Acc\", linestyle='-.')\n",
    "plt.title(\"Fine-tune Training Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, \"finetune_accuracy_curve.png\"))\n",
    "plt.close()\n",
    "print(\"‚úÖ Saved: finetune_accuracy_curve.png\")\n",
    "\n",
    "# Full train accuracy (merged)\n",
    "full_acc = full_history[\"accuracy\"]\n",
    "full_val_acc = full_history[\"val_accuracy\"]\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(full_acc, label=\"Train Acc\", color='black', alpha=0.7)\n",
    "plt.plot(full_val_acc, label=\"Val Acc\", color='red', alpha=0.7)\n",
    "plt.title(\"Full Training Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, \"full_accuracy_curve.png\"))\n",
    "plt.close()\n",
    "print(\"‚úÖ Saved: full_accuracy_curve.png\")\n",
    "\n",
    "# --- Loss Plots ---\n",
    "# Head loss\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(history_head.history[\"loss\"], label=\"Train Loss\", linestyle='--')\n",
    "plt.plot(history_head.history[\"val_loss\"], label=\"Val Loss\", linestyle='--')\n",
    "plt.title(\"Head Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, \"head_loss_curve.png\"))\n",
    "plt.close()\n",
    "print(\"‚úÖ Saved: head_loss_curve.png\")\n",
    "\n",
    "# Fine-tune loss\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(history_finetune.history[\"loss\"], label=\"Train Loss\", linestyle='-.')\n",
    "plt.plot(history_finetune.history[\"val_loss\"], label=\"Val Loss\", linestyle='-.')\n",
    "plt.title(\"Fine-tune Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, \"finetune_loss_curve.png\"))\n",
    "plt.close()\n",
    "print(\"‚úÖ Saved: finetune_loss_curve.png\")\n",
    "\n",
    "# Full train loss (merged)\n",
    "full_loss = full_history[\"loss\"]\n",
    "full_val_loss = full_history[\"val_loss\"]\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(full_loss, label=\"Train Loss\", color='black', alpha=0.7)\n",
    "plt.plot(full_val_loss, label=\"Val Loss\", color='red', alpha=0.7)\n",
    "plt.title(\"Full Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, \"full_loss_curve.png\"))\n",
    "plt.close()\n",
    "print(\"‚úÖ Saved: full_loss_curve.png\")\n",
    "\n",
    "# # Merge histories\n",
    "full_history = {}\n",
    "for key in set(history_head.history) | set(history_finetune.history):\n",
    "    full_history[key] = history_head.history.get(key, []) + history_finetune.history.get(key, [])\n",
    "\n",
    "# --- Accuracy ---\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(history_head.history[\"accuracy\"], label=\"Head Train Acc\", linestyle='--')\n",
    "plt.plot(history_head.history[\"val_accuracy\"], label=\"Head Val Acc\", linestyle='--')\n",
    "plt.plot(history_finetune.history[\"accuracy\"], label=\"Fine-tune Train Acc\", linestyle='-.')\n",
    "plt.plot(history_finetune.history[\"val_accuracy\"], label=\"Fine-tune Val Acc\", linestyle='-.')\n",
    "plt.plot(full_history[\"accuracy\"], label=\"Full Train Acc\", color='black', alpha=0.3)\n",
    "plt.plot(full_history[\"val_accuracy\"], label=\"Full Val Acc\", color='red', alpha=0.3)\n",
    "plt.title(\"Learning Curve - Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, \"cumulative_learning_curve_accuracy.png\"))\n",
    "plt.close()\n",
    "print(\"‚úÖ Saved: cumulative_learning_curve_accuracy.png\")\n",
    "\n",
    "# --- Loss ---\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(history_head.history[\"loss\"], label=\"Head Train Loss\", linestyle='--')\n",
    "plt.plot(history_head.history[\"val_loss\"], label=\"Head Val Loss\", linestyle='--')\n",
    "plt.plot(history_finetune.history[\"loss\"], label=\"Fine-tune Train Loss\", linestyle='-.')\n",
    "plt.plot(history_finetune.history[\"val_loss\"], label=\"Fine-tune Val Loss\", linestyle='-.')\n",
    "plt.plot(full_history[\"loss\"], label=\"Full Train Loss\", color='black', alpha=0.3)\n",
    "plt.plot(full_history[\"val_loss\"], label=\"Full Val Loss\", color='red', alpha=0.3)\n",
    "plt.title(\"Learning Curve - Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, \"cumulative_learning_curve_loss.png\"))\n",
    "plt.close()\n",
    "print(\"‚úÖ Saved: cumulative_learning_curve_loss.png\")\n",
    "\n",
    "# ===============================\n",
    "# DONE\n",
    "# ===============================\n",
    "print(\"\\n‚úÖ K-Fold training and evaluation completed.\")\n",
    "print(\"‚úÖ All evaluation results and plots saved in:\", save_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
